# -*- coding: utf-8 -*-
"""토크나이저 test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z4HsL9biFKeVjxdFYq86xWIq6ic79Dtp
"""

!pip install konlpy

# Commented out IPython magic to ensure Python compatibility.
#install Mecab
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
# %cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh

# 테스트 할 텍스트 입력
text = "라이온즈 파크에 3년만에 왔다. 그동안 삼성이 못해서 직관을 안왔는데 요즘 야구를 잘하니 치맥을 먹을 맛이 났다."

from konlpy.tag import Okt, Mecab, Komoran, Kkma, Hannanum
mecab = Mecab()
okt = Okt()
kkm = Kkma()
kom = Komoran()
han = Hannanum()


okt_nouns = okt.nouns(text)
mecab_nouns = mecab.nouns(text)
kkm_nouns = kkm.nouns(text)
kom_nouns = kom.nouns(text)
han_nouns = han.nouns(text)

#한글자 제거 함수 정의
def biggerthanlen2(x):
    return len(x)>1

#적용
texthashtagOkt = list(filter(biggerthanlen2,okt_nouns))
texthashtagMecab = list(filter(biggerthanlen2,mecab_nouns))
texthashtagKkma = list(filter(biggerthanlen2,kkm_nouns))
texthashtagKomoran = list(filter(biggerthanlen2,kom_nouns))
texthashtagHannanum = list(filter(biggerthanlen2,han_nouns))

print("Okt를 사용한 결과:",texthashtagOkt)
print("Mecab을 사용한 결과:",texthashtagMecab)
print("Kkma를 사용한 결과:",texthashtagKkma)
print("Komoran을 사용한 결과:",texthashtagKomoran)
print("Hannanum을 사용한 결과:",texthashtagHannanum)

!pip install transformers
import torch
from transformers import PreTrainedTokenizerFast
from transformers import BartForConditionalGeneration

tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')
model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')


text = "라이온즈 파크에 3년만에 왔다. 그동안 삼성이 못해서 직관을 안왔는데 요즘 야구를 잘하니 치맥을 먹을 맛이 났다."

raw_input_ids = tokenizer.encode(text)
input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]

summary_ids = model.generate(torch.tensor([input_ids]))
tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)

textresult = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)
print(textresult)

okt_nouns = okt.nouns(textresult)
mecab_nouns = mecab.nouns(textresult)
kkm_nouns = kkm.nouns(textresult)
kom_nouns = kom.nouns(textresult)
han_nouns = han.nouns(textresult)

#한글자 제거 함수 정의
def biggerthanlen2(x):
    return len(x)>1

#적용
texthashtagOkt = list(filter(biggerthanlen2,okt_nouns))
texthashtagMecab = list(filter(biggerthanlen2,mecab_nouns))
texthashtagKkma = list(filter(biggerthanlen2,kkm_nouns))
texthashtagKomoran = list(filter(biggerthanlen2,kom_nouns))
texthashtagHannanum = list(filter(biggerthanlen2,han_nouns))

print("Okt를 사용한 결과:",texthashtagOkt)
print("Mecab을 사용한 결과:",texthashtagMecab)
print("Kkma를 사용한 결과:",texthashtagKkma)
print("Komoran을 사용한 결과:",texthashtagKomoran)
print("Hannanum을 사용한 결과:",texthashtagHannanum)